\section{Solving large-scale eigenvalue problems}

\begin{tcolorbox}[
        title=Eigenvalue problem
    ]
    Given a matrix $A \in \mathbb{C}^{n \times n}$, find $(\lambda, \mathbf{v}) \in \mathbb{C} \times \mathbb{C}^n \setminus \{ \mathbf 0 \}$ such that
    $$
        A \mathbf{v} = \lambda \mathbf{v}
    $$
    where $\lambda$ is an eigenvalue of $A$,
    and $\mathbf v$ (non-zero) is the corresponding eigenvector.
\end{tcolorbox}

The set of all the eigenvalues of a matrix $A$ is calle the \textbf{spectrum} of $A$.

The maximum modulus of all the eigenvalues is called the \textbf{spectral radius} of $A$:
$\rho(A) = \max \{ |\lambda| : \lambda \in \lambda(A) \}$.

The problem $A \mathbf v = \lambda \mathbf v$ is equivalent to $(A - \lambda I) \mathbf v = 0$.
$\det (A - \lambda I) = 0$ is a polynomial of degree $n$ in $\lambda$:
it is called the \textbf{characteristic polynomial} of $A$ and its roots are the eigenvalues of $A$.

\subsection{Similarity transformations}

We first need to identify what types of transformations preserve eigenvalues,
and for what types of matrices the eigenvalues are easily determined.

\begin{tcolorbox}[title=Definition]
    The matrix $B$ is similar to the matrix $A$ if there exists a nonsingulat matrix $T$ such taht $B = T^{-1} A T$.
\end{tcolorbox}

With the above definition, it is trivial to show that
$$
    B \mathbf y = \lambda \mathbf y
    \implies
    T^{-1} A T \mathbf y = \lambda \mathbf y
    \implies
    A(T \mathbf y) = \lambda (T \mathbf y)
$$
so that $A$ and $B$ have the same eigenvalues,
and if $\mathbf y$ is an eigenvector of $B$,
then $\mathbf v = T\mathbf y$ is an eigenvector of $A$.

Similarity transformations preserve eigenvalues but do not preserve eigenvectors.

\subsection{The power method}

The power method is the simplest method for computing a single eigenvalue and eigenvector of a matrix.

Assume that the matrix $A$ has a unique eigenvalue
$\lambda_1$ of maximum modulus,
i.e., $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq \dots \geq |\lambda_n|$,
with corresponding eigenvector $\mathbf v_1$.

Starting from a given nonzero vector $\mathbf x^{(0)}$ such that $\lVert \mathbf x^{(0)} \rVert = 1$,
let's consider the following iteration scheme, for $k >= 0$:
\begin{align*}
     & \mathbf y^{(k + 1)} \gets A \mathbf x^{(k)}                                         \\
     & \mathbf x^{(k+1)} \gets \frac{\mathbf y^{(k + 1)}}{\lVert \mathbf y^{(k+1)} \rVert} \\
     & \nu^{(k+1)} \gets [\mathbf x^{(k+1)}]^H A \mathbf x^{(k+1)}
\end{align*}

It can be shown that the above iteration scheme converges to a
multiple of $\mathbf v_1$, the eigenvector corresponding to the dominant eigenvalue $\lambda_1$.

The convergence rate of the power method depends on the ratio $|\lambda_2| / |\lambda_1|$,
where $\lambda_2$ is the eigenvalue having the second largest modulus. The smaller $|\lambda_2| / |\lambda_1|$,
the faster the convergence is.

\subsubsection{Deflation methods}

Suppose that an eigenvalue $\lambda_1$ and corresponding eigenvector
$\mathbf v_1$ for a matrix $A$ have been computed.
We can compute additional eigenvalues $\lambda_2, \dots, \lambda_n$ of $A$ by a process called \textbf{deflation},
which removes the known eigenvalue.

The \textit{idea} is to construct a new matrix $B$ with eigenvalues $\lambda_2, \dots, \lambda_n$
that deflates the matrix $A$, removing $\lambda_1$.
Then $\lambda_2$ can be obtained by the power method.

Let $S$ be any nonsingular matrix such that
\begin{itemize}
    \item $S \mathbf v_1 = \alpha \mathbf e_1$
    \item $S$ is a scalar multiple of the first column $\mathbf e_1$ of the identity matrix $I$.
\end{itemize}
Then the similarity transformation determined by $S$ transforms $A$ into the from
$$
    SAS^{-1} = \begin{pmatrix}
        \lambda_1 & \mathbf b^T \\ 0 & B
    \end{pmatrix}
$$
We use $B$ to compute the next eigenvalue $\lambda_2$ and eigenvector $\mathbf z_2$.
Given $\mathbf z_2$ eigenvector of $B$, we want to compute the second eigenvector
$\mathbf v_2$ of the matrix $A$.
We just need to add an element to the vector $\mathbf z_2$ (that consists of $n-1$ elements),
that is
$$
    \mathbf v_2 = S^{-1} \begin{pmatrix}
        \alpha &  & \mathbf z_2
    \end{pmatrix}
    \qquad
    \alpha = \frac{\mathbf b^H \mathbf z_2}{\lambda_1 - \lambda_2}
$$
The process can be repeated to find the additional eigenvalues and eigenvectors.

\subsubsection{Inverse power method}

For some applications,
the smallest eigenvalue of a matrix is required rather than the largest.
We use the fact that the eigenvalues of $A^{-1}$ are the reciprocals
of those of $A$ (hence the smallest eigenvalue of $A$
is the reciprocal of the largest eigenvalue of $A^{-1}$)

Starting form a given nonzero vector $\mathbf q^{(0)}$ such that
$\lVert \mathbf q^{(0)} \rVert = 1$, let's consider the following iteration scheme,
for $k \geq 0$:

\begin{align*}
     & \text{Solve } A \mathbf z^{(k + 1)} =\mathbf q^{(k)}                                  \\
     & \mathbf q^{(k + 1)} \gets \frac{\mathbf z^{(k+1)}}{\lVert \mathbf z^{(k + 1)}} \rVert \\
     & \sigma^{(k+1)} \gets [\mathbf q^{k+1}]^H A \mathbf q^{(k+1)}
\end{align*}

\subsubsection{Inverse power method with shift}

We want to approximate the eigenvalue $\lambda$ of $A$ which is the closest
to a given number $\mu \notin \sigma(A)$.
We introduce $M_\mu = A - \mu I$ and observe that the eigenvalue $\lambda$ of $A$
which is closest to $\mu$ is the minimum eigenvalue of $M_\mu$.

Starting from a given nonzero vector $\mathbf q^{(0)}$ such that $\lVert \mathbf q^{(0)} \rVert = 1$,
let's consider the following iteration scheme, for $k \geq 0$:
\begin{align*}
     & \text{Solve } M_\mu \mathbf z^{(k+1)} = \mathbf q^{(k)}                           \\
     & \mathbf q^{(k+1)} \gets \frac{\mathbf z^{(k+1)}}{\lVert \mathbf z^{(k+1)} \rVert} \\
     & \nu^{(k+1)} \gets [\mathbf q^{(k+1)}]^H A \mathbf q^{(k+1)}
\end{align*}

\subsection{QR factorization}

\subsubsection{Projectors and complementary projectors}

A projector is a square matrix $P \in \mathbb{R}^{n \times n}$ that satisfies $P^2 = P$:

\begin{itemize}
    \item If $\mathbf w \in \mathrm{range} (P)$, then $P \mathbf w = \mathbf w$.
          Indeed, since $\mathbf w \in \mathrm{range}(P)$,
          then $\mathbf w = P \mathbf z$,
          for some $\mathbf z$. Therefore:
          $$
              P \mathbf w = P (P \mathbf z) = P^2 \mathbf z = P \mathbf z = \mathbf w
          $$
    \item The matrix $I -P$ is the complementary projector to $P$.
    \item $I - P$ projects on the nullspace of $P$:
          if $P \mathbf w = 0$, then $(I - P) \mathbf w = \mathbf w$,
          so $\mathrm{null}(P) \subseteq \mathrm{range}(I - P)$.
          But for any $\mathbf w$, $(I - P) \mathbf w = \mathbf w - P \mathbf w \in \mathrm{null}(P)$,
          so $\mathrm{range}(I - P) \subseteq \mathrm{null}(P)$.
          Therefore,
          $$ \mathrm{range}(I - P) = \mathrm{null}(P) $$
          and
          $$ \mathrm{null}(I - P) = \mathrm{range}(P) $$
    \item A projector $P$ is \textit{orthogonal} if $P = P^2 = P^T$.
\end{itemize}